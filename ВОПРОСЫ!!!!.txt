1. Модели данных и нормализация:

1.1 Что такое нормализация данных?

       Нормализация данных — это процесс организации данных в базе данных для уменьшения избыточности и улучшения их целостности.
       Основные цели нормализации:
            1) Устранение дублирования данных (хранение одной информации в одном месте).
            2) Минимизация аномалий при вставке, обновлении и удалении (например, избежание противоречивых данных).
            3) Упрощение структуры базы данных для удобства поддержки и масштабирования.

1.2 Какие бывают модели данных?

        1) Реляционная модель (SQL)
        2) Документоориентированная модель (NoSQL)
        3) Ключ-значение (Key-Value) (Пример: Redis, DynamoDB, Memcached)
        4) Колоночная (Column-family) (Пример: Cassandra, HBase, Google BigTable)
        5) Многомерная модель (OLAP) (Пример: ClickHouse, Microsoft Analysis Services)

1.3 Что такое Data Vault?

        Сочетает преимущества нормализованных и денормализованных подходов,
         ориентируясь на управление изменениями и аудит данных.
        Преимущества:
            1) Гибкость
            2) Масштабируемость
        Недостатки:
            1) Сложность
            2) Не совсем хорошо работает на практике
        Состоит из 3 элементов:
            1) Хабы - таблицы с уникальными ключами, только айдишники
            2) Линки - таблицы с бизнес-ключами разных таблиц
            3) Сателлиты - описательная характеристика хабов

1.4 Что такое DWH по Инмону и Кимбалу?

        1) По Кимбалу:
            Хранилище строится как набор тематических витрин данных (Data Marts) в звездообразной схеме (Star Schema).
            Данные сразу трансформируются в оптимизированный для анализа вид (денормализация).
            Согласованные измерения (Conformed Dimensions) обеспечивают целостность между витринами.

            Для СКОРОСТИ

        2) По Инмону:
            Единое централизованное хранилище (EDW — Enterprise Data Warehouse), основанное на нормализованной модели (3NF).
            Данные загружаются из источников в DWH без избыточной трансформации (сохраняется детализация).
            Витрины данных (Data Marts) создаются на основе DWH как денормализованные представления.

            Для ГИБКОСТИ

2. Потоковая и пакетная обработка данных

2.1  Что такое потоковая обработка данных и пакетная обработка данных?

        Потоковая - Данные обрабатываются непрерывно, по мере их поступления,
         в реальном времени или с минимальной задержкой.
        Технологии:
            1) Apache Kafka (передача потоковых данных).
            2) Apache Flink, Apache Storm, Spark Streaming (обработка).
            3) AWS Kinesis, Google Pub/Sub.

        Пакетная - Данные накапливаются и обрабатываются партиями (batch)
         через заданные интервалы времени.

        Технологии:
            1) Apache Hadoop (HDFS + MapReduce).
            2) Apache Spark (оптимизированная пакетная обработка).
            3) SQL-СУБД (PostgreSQL, Oracle).

3. ClickHouse

3.1 Как работает ClickHouse?

        ClickHouse — это колоночная СУБД с открытым исходным кодом,
        разработанная Yandex для высокопроизводительной аналитики больших данных (OLAP).
         Она оптимизирована для быстрых агрегаций и обработки запросов на огромных объемах данных.

         1) Колоночное хранение – Данные хранятся по столбцам, а не строкам (как в классических SQL-СУБД).
                Это ускоряет агрегацию (SUM, AVG) и уменьшает объем чтения с диска.
         2) Высокая скорость – Запросы выполняются за миллисекунды даже на терабайтах данных благодаря:
         3) Сжатие данных – Эффективные алгоритмы (LZ4, ZSTD) уменьшают объем хранилища в 5–10 раз.
         4) Поддержка SQL – ClickHouse понимает стандартный SQL (+ расширения для аналитики).
         5) Горизонтальная масштабируемость – Распределенные запросы между серверами (шардирование и репликация).

3.2 Что такое колоночная база данных, в чем ее преимущества и недостатки?

         Колоночная база данных - данные хранятся не в построчной таблице, а в колоночной.

            Предназначена для OLAP, то есть для быстрой аналтики, характеризуется высокой скоростью
         счёта различных метрик, подходит для таблиц с млрд. строк.
            Медленные вставки, удаления, изменения таблиц - не предназначена для OLTP,
         Подходят для:
            1) Аналитика и отчетность (DWH, BI-системы).
            2) Обработка логов (ClickHouse, Apache Druid).
            3) Сбор метрик (мониторинг, A/B-тесты).
            4) Сложные агрегации (финансовые расчеты, ML-фичи).
        Не подходят для:
            1) Транзакционные системы (интернет-магазины, банковские операции).
            2) Частые UPDATE/DELETE.
            3) Случаи, где нужны точечные выборки по первичному ключу.

3.3 Отличия OLAP vs OLTP

    Критерий	         OLTP	                                    OLAP
    Цель	             Обработка транзакций в реальном времени	Анализ больших объемов исторических данных
    Тип запросов	     INSERT, UPDATE, DELETE, простые SELECT	    Сложные SELECT с агрегациями (SUM, AVG, GROUP BY)
    Скорость операций	 Миллисекунды	                            Секунды/минуты
    Объем данных	     Тысячи/миллионы строк	                    Миллиарды строк
    Структура данных	 Нормализованная (3NF)	                    Денормализованная (звезда/снежинка)
    Примеры СУБД	     MySQL, PostgreSQL, Oracle	                ClickHouse, Amazon Redshift, Snowflake

3.4 Основные движки ClickHouse

    1) Для аналитики и больших данных → MergeTree / ReplicatedMergeTree.
    2) Для временных данных → Log / TinyLog.
    3) Для интеграции с Kafka → Kafka.
    4) Для работы с внешними БД → MySQL / PostgreSQL.
    5) Для хранения в оперативке → Memory.

4. Python

4.1 Какие бывают магические функции в Python?

    ① Инициализация и управление объектом
        1) __init__(self, ...) — конструктор (вызывается при создании объекта).
        2) __new__(cls, ...) — создаёт экземпляр (используется для кастомного управления созданием объектов).
        3) __del__(self) — деструктор (вызывается перед удалением объекта).
    ② Строковое представление
        1) __str__(self) → str(obj) / print(obj) (читаемое представление).
        2) __repr__(self) → repr(obj) (однозначное представление, используется в REPL).
        3_ __format__(self, format_spec) → format(obj, spec).

4.2 Что такое декораторы в Python?

    Декоратор — это функция, которая:
        1) Принимает функцию/класс в качестве аргумента.
        2) Добавляет дополнительную логику (например, логирование, кэширование, проверку прав доступа).
        3) Возвращает новую функцию или заменяет исходную.

    ==def logger(func):
        def wrapper(*args, **kwargs):
            print(f"Вызов функции {func.__name__} с аргументами {args}, {kwargs}")
            result = func(*args, **kwargs)
            print(f"Функция {func.__name__} вернула {result}")
            return result
        return wrapper

        @logger
        def add(a, b):
            return a + b

        add(2, 3) ==

    Вызов функции add с аргументами (2, 3), {}
    Функция add вернула 5

4.3 Что такое try except else finally

        1) try — код, который может "упасть".
        2) except — обработка ошибок.
        3) else — выполняется, если ошибок не было.
        4) finally — выполняется всегда (даже при return или исключении)

4.4 Чем отличается итератор от генератора?

        Итератор — это интерфейс для перебора элементов.
        Генератор — это упрощённая и более эффективная реализация итератора.

        Генераторы идеальны для работы с потоками данных, а итераторы — для кастомных сценариев перебора.

        Запомните:
            1) Все генераторы — итераторы, но не все итераторы — генераторы.
            2) Генераторы используют yield, итераторы — __next__().
            3) Если нужно быстро создать последовательность — используйте генератор.
               Если нужен полный контроль над итерацией — пишите итератор. 🚀

4.5 # Что выведет print(a)?
a = [1,2,3]
b = a
b.append(4)

        [1,2,3,4] (что бы было [1,2,3] -- b = a.copy())

4.6 Какие типы данных в Python являются изменяемыми и неизменяемыми?

        НеизменяемыУ:
            1) Числа
            2) Строки
            3) Кортежи tuple
            4) Фрозен-множества frozenset
        Изменяемые:
            1) Списки
            2) Множества
            3) Словари (только value)


4.7 Как работает память в питоне?

        Куча (Heap) — хранит все объекты Python (например, списки, словари, экземпляры классов). (5)
        Стек (Stack) — содержит ссылки на объекты из кучи и управляет вызовами функций. (x)

        Всё, абсолютно всё в питоне - объекты!!!!!!!!!!

4.8 Как работает сборщик мусора в python?

        Основной механизм — счётчик ссылок (быстрый, но не обрабатывает циклы).
        Дополнительный механизм — поколенческий GC (ищет и удаляет циклические ссылки).

            Оптимизация:
                1) Для высоконагруженного кода можно отключать GC.
                2) Используйте weakref для сложных связей.

        Важно: В большинстве случаев Python сам управляет памятью, но понимание работы GC помогает избегать утечек! 🚀

4.9 Написать сортировка пузырьком, быструю сортировку, сортировку слиянием

        Для маленьких массивов (до 100 элементов) — Bubble Sort (из-за простоты).
        Для общего случая — Quick Sort (обычно быстрее на практике).
        Если важна стабильность и гарантированное время — Merge Sort.

4.10 Можно ли в словарь в key записать изменяемый тип? Почему?

        Нет. Ключ словаря должен быть неизменяемым (immutable) типом

5 Apache Spark

5.1 Почему нельзя использовать Pandas для больших данных, а нужно использовать Spark?

        Pandas работает на локальной машине и предназначен для работы с CSV, Excel
        Spark же работает на кластерах (несколько серверов), сочитаем с HDFS, PARQUET, S3,
        Разделяет данные на партиции


5.2 Минимальное параллелизм в Spark и что это такое?

        Параллелизм в Spark — это способность выполнять несколько задач одновременно на кластере.
        Spark использует модель параллелизма на уровне операций, что означает, что
        каждая операция в коде может быть выполнена параллельно на разных узлах кластера.

5.3 Что такое RDD в Spark?

        RDD – это разновидность датасета (простого набора данных),
        который разделен на множество машин, работающих в кластере.

5.4 Что такое Dataset и чем отличается от dataframe и RDD?

        DataFrame — лучший выбор для большинства задач (оптимизирован, удобен).
        RDD — для низкоуровневых операций.
        Dataset — аналог DataFrame с типобезопасностью (только в Scala/Java).

    Важно: В PySpark нет Dataset, но его заменяет DataFrame (который в Scala — это Dataset[Row])

5.5 Какие виды кэширования существуют в Spark и чем они отличаются?

5.6 Что такое persist в Spark и какие storage levels существуют?

5.7 Чем отличается repartition от coalesce в Spark?

5.8 Что делает YARN и зачем он нужен?

5.9 Какие настройки Spark applications вы используете?

5.10 Сколько гигабайт памяти выделяется на каждую задачу в Spark?

5.11 Что такое spill в Spark?

5.12 Что такое broadcast join в Spark и как его настроить?

5.13 Что такое ленивые вычисления в Spark?

5.14 Что такое Adaptive query execution?

6. Apache Airflow

6.1 Сколько типов сервисов в Docker при разворачивании Airflow?

6.2 Чем отличается Celery Executor local executor

6.3 Какая база данных используется в Airflow?

6.4 Можно ли передавать данные по XCOM?

7. HDFS

7.1 Что такое HDFS блоки и какие у них есть минусы?

7.2 Как бороться с маленькими файлами в HDFS? Переполнение NameNode